{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 7153077,
          "sourceType": "datasetVersion",
          "datasetId": 4130355
        }
      ],
      "dockerImageVersionId": 30626,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installations"
      ],
      "metadata": {
        "id": "mm6Sqpyponxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts-hu4Kdr78V",
        "outputId": "23d9db43-39c4-48af-ba48-5af9c3781e1f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "%pip install \\\n",
        "    transformers==4.27.2 \\\n",
        "    datasets==2.11.0  --quiet"
      ],
      "metadata": {
        "id": "i-Qlaj70t7by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importings"
      ],
      "metadata": {
        "id": "-RXhXyQ8uc29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GenerationConfig"
      ],
      "metadata": {
        "id": "lY-TY6yft2bA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Sample"
      ],
      "metadata": {
        "id": "J6jM4p19onxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv1 = '''\n",
        "\n",
        "User: Hi there! I'm interested in exchanging my laptop for a newer model. It's in good condition, but I need something with better performance for my work. Do you have any options available?\n",
        "\n",
        "Seller: Hello! Sure, we have a variety of laptops with different specifications. Can you provide more details about your current laptop, such as the model, specifications, and any additional features you're looking for in the new one?\n",
        "\n",
        "User: Of course. It's a mid-range laptop with an Intel Core i5 processor, 8GB RAM, and a 256GB SSD. I'm looking for something with a more powerful processor and larger storage capacity.\n",
        "\n",
        "Seller: Great! We have several options that meet your criteria. Let me show you a few models and discuss the possibility of a trade-in discount for your current laptop.'''\n",
        "sum1 = \"\"\"The user initiates a conversation expressing interest in exchanging their current laptop for a newer model with better performance. The seller engages in a dialogue, seeking details about the user's current laptop and their preferences for the new one. The user specifies the specifications they have and the desired upgrades. The seller then offers to show various laptop options and discusses the potential for a trade-in discount, aiming to facilitate a smooth product exchange.\"\"\"\n",
        "\n",
        "conv2=\"\"\"User: Hi, I recently purchased a smartphone from your store, and I'm not satisfied with its performance. I've been experiencing frequent glitches and slow response times. Is there a possibility to return or exchange it for a different model?\n",
        "\n",
        "Customer Service: Hello! I'm sorry to hear about the issues you're facing. We value your feedback. Can you provide more details about the specific problems you're encountering with the smartphone?\n",
        "\n",
        "User: Certainly. The phone freezes often, and the applications take a long time to load. I've tried troubleshooting, but the issues persist, affecting my overall experience with the device.\n",
        "\n",
        "Customer Service: I understand your concerns. Let me check our return and exchange policy for you. It typically involves a certain timeframe for returns and exchanges, and we may need to assess the device's condition. Have you kept the original packaging and accessories?\"\"\"\n",
        "\n",
        "sum2=\"\"\"\n",
        "The user expresses dissatisfaction with a recently purchased smartphone and inquires about the possibility of a return or exchange due to performance issues. The customer service representative engages in a conversation, asking for specific details about the problems. The user describes freezing and slow response times. The representative acknowledges the concerns and mentions the return and exchange policy, indicating a need to assess the device's condition. The user's request for resolution is met with an exploration of the store's procedures for handling such cases.\"\"\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-19T15:09:36.569409Z",
          "iopub.execute_input": "2023-12-19T15:09:36.570071Z",
          "iopub.status.idle": "2023-12-19T15:09:36.576852Z",
          "shell.execute_reply.started": "2023-12-19T15:09:36.570033Z",
          "shell.execute_reply": "2023-12-19T15:09:36.575564Z"
        },
        "trusted": true,
        "id": "09TZvAD8onxl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flan-T5 LLM - Model & Tokenizer\n",
        "\n",
        "**Flan-T5** is an instruction-tuned version of **T5**, a popular encoder-decoder model. It is capable of performing various zero-shot NLP tasks such as text summarization, common sense reasoning, natural language inference, question answering, sentence and sentiment classification, translation, and pronoun resolution with appropriate prompting.\n",
        "\n",
        "Flan-T5 is an instruction-tuned model and is capable of performing few-shot in-context learning tasks. The **Flan-T5** family of models is used in Amazon's Amazon Mechanical Turk platform."
      ],
      "metadata": {
        "id": "OE-2iLjGonxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Encodeing and decoding for sample sentence"
      ],
      "metadata": {
        "id": "msaiy6BAonxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='google/flan-t5-base'\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "id": "b7KXh3EjupA-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"A large language model (LLM) is a type of artificial intelligence (AI) algorithm that uses deep learning techniques\"\n",
        "#sentence = \"What time is it, Tom?\"\n",
        "\n",
        "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
        "\n",
        "sentence_decoded = tokenizer.decode(\n",
        "        sentence_encoded[\"input_ids\"][0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "print('ENCODED SENTENCE:')\n",
        "print(sentence_encoded[\"input_ids\"][0])\n",
        "print('\\nDECODED SENTENCE:')\n",
        "print(sentence_decoded)\n"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNjzN5b_onxm",
        "outputId": "cc49bf83-583a-4c3c-843e-dcc761938453"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENCODED SENTENCE:\n",
            "tensor([   71,   508,  1612,   825,    41, 10376,   329,    61,    19,     3,\n",
            "            9,   686,    13,  7353,  6123,    41,  9822,    61, 12628,    24,\n",
            "         2284,  1659,  1036,  2097,     1])\n",
            "\n",
            "DECODED SENTENCE:\n",
            "A large language model (LLM) is a type of artificial intelligence (AI) algorithm that uses deep learning techniques\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering"
      ],
      "metadata": {
        "id": "HF3j4diyonxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_generation(model,prompt,config = None):\n",
        "    tokenized = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors = 'pt',\n",
        "    ).input_ids\n",
        "    if config is not None:\n",
        "        generation=model.generate(\n",
        "        tokenized,\n",
        "        generation_config = generation_config)[0]\n",
        "    else:\n",
        "        generation = model.generate(\n",
        "            tokenized,\n",
        "            max_new_tokens = 50\n",
        "        )[0]\n",
        "    decoded = tokenizer.decode(\n",
        "        generation,\n",
        "        skip_special_tokens = True\n",
        "    )\n",
        "    return decoded"
      ],
      "metadata": {
        "trusted": true,
        "id": "vk_v_1_Zonxm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_generation(model=model,prompt=conv1)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BdfwZ-Yfonxm",
        "outputId": "9ef10390-4905-436d-aa36-d12f665c71f5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Seller: Sure!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Instructional Prompts\n",
        "\n",
        "In our earlier evaluation, we recognized the model's inherent capabilities; however, it struggled to grasp the core essence of the task at hand.\n",
        "\n",
        "Now, we're equipped to bridge that understanding gap through a powerful technique called Instruction Fine-Tuning. Within this approach lies the art of Prompt Engineering and the finesse of Few-Shot Inferences. These tools will enable us to guide and support the model, ensuring it not only comprehends the task but excels in delivering precise and context-aware responses.\n",
        "\n",
        "add Codeadd Markdown"
      ],
      "metadata": {
        "id": "UmWC0UK-onxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero Shot Inference\n",
        "\n",
        "Without providing any task-specific examples, we entrust the model with the task, expecting it to autonomously comprehend the problem and generate the desired outcomes. It's a test of the model's inherent understanding and adaptability."
      ],
      "metadata": {
        "id": "RukdtHc7onxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Dialog:\n",
        "\n",
        "{conv1}\n",
        "\n",
        "Whats going on?\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "id": "IBV5Ip5Eonxn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_completion = model_generation(model=model,prompt=prompt)"
      ],
      "metadata": {
        "id": "kn5Z4N8nonxn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_completion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TIICENEgtTWH",
        "outputId": "8a6e0ade-2e37-4efd-a835-736941af6f9e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"User: Hi there! I'm interested in trading in my laptop for a newer model.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One Shot Learning"
      ],
      "metadata": {
        "id": "_-Eu0woronxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#my code\n",
        "prompt=f\"\"\"\n",
        "Dialog:\n",
        "{conv1}\n",
        "\n",
        "what's going on ?\n",
        "{sum1}\n",
        "\n",
        "Dialog:\n",
        "{conv2}\n",
        "\n",
        "what's going on ?\n",
        "\"\"\"\n",
        "model_completion = model_generation(model = model,prompt=prompt)"
      ],
      "metadata": {
        "id": "aW4rlCvuonxn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_completion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "P0j1TWKotcis",
        "outputId": "cbe9f0e5-d156-4f18-f2d3-36701c69699b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"User: I'm not satisfied with the performance of the smartphone. I've tried troubleshooting, but the issues persist. Customer Service will return or exchange the device.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above results we can say that one shot is giving comparitively better results than zero shot."
      ],
      "metadata": {
        "id": "XZW9cdMlut7V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vU5r2gDGuHie"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}